# 用CNN实现

```python
#1
import torch 
import torch.nn as nn
import torch.nn.functional as F 
#用来制作Dataloader
import torch.utils.data as Data 
import torch.optim as optim
#MNIST数据集哎torchvision里面
import torchvision

#2
#训练集一共有60000张图片 使用小批量梯度下降 batch_size代表一次将64张图片输入到神经网络(每张图片大小是1*28*28)
batch_size = 64
transform = torchvision.transforms.Compose([
    #将MNIST中的数据转换为tensor
    torchvision.transforms.ToTensor(),
    #切换到标准差分布
    torchvision.transforms.Normalize((0.1307, ), (0.3081, ))
])

train_data = torchvision.datasets.MNIST('./dataset/mnist/', train= True, transform= transform)
train_loader = Data.DataLoader(train_data, shuffle= True, batch_size= batch_size)
test_date = torchvision.datasets.MNIST('./dataset/mnist/', train= False, transform= transform)
test_loader = Data.DataLoader(test_date, shuffle= False, batch_size= batch_size)

#3
class CNN(nn.Module):
    def __init__(self):
        super(CNN,self).__init__()
        #参数分别是:输入的维度(一层,灰度图), 输出的维度(10层的feature map,用了10个kernel), kernel的大小
        self.conv1 = nn.Conv2d(1,10,5)
        self.conv2 = nn.Conv2d(10,20,5)
        #maxpooling的大小是2(2*2中选取最大的)
        self.pooling = nn.MaxPool2d(2)
        #全连接网络
        self.fc = nn.Linear(320,10)
        
    def forward(self,x):
        #这一步很重要 batch_size是64 训练集一共有60000张照片 60000%64=32 所以最后一个batch里只有32张照片 
        batch_size = x.size(0)
        #输入大小: batch_size*1*28*28 输出大小: batch_size*10*24*24(10个kernel,28-5+1=24) maxpooling之后: batch_size*10*12*12
        x = F.relu(self.pooling(self.conv1(x)))
        #输入大小: batch_size*10*12*12 输出大小: batch_size*20*8*8 maxpooling之后:batch_size*20*4*4
        x = F.relu(self.pooling(self.conv2(x)))
        #输入大小: batch_size*20*4*4 经过view函数reshape之后: batch_size*320(这也是为什么Linear(320,10))
        x = x.view(batch_size,-1)
        x = self.fc(x)
        return x
model = CNN()

#4
#使用交叉熵作为损失函数
criterion = nn.CrossEntropyLoss()
#用mini-batch stochastic gradient descent(小批量随机梯度下降)
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum= 0.5)

#5
def train(epoch):
    running_loss = 0
    for batch_idx, data in enumerate(train_loader):
        inputs, target = data
        #将参数清零
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()
		
        #为了不构建向量图 取loss.item() 是一个标量
        running_loss += loss.item()
        if batch_idx % 300 ==299:
            print("[%d,%5d] loss: %.3f" % (epoch+1, batch_idx+1, running_loss/300))
            running_loss = 0

#6
def test():
    correct = 0
    total = 0
    #当您确定不会调用时，禁用梯度计算对于推断很有用Tensor.backward()。它将减少原本需要require_grad = True的计算的内存消耗。在这种模式下，即使输入具有require_grad = True，每次计算的结果也将具有 require_grad = False。(来自pytorch文档)
    with torch.no_grad():
        for data in test_loader:
            inputs, labels = data
            outputs = model(inputs)
            #对于每个图像,有10个数字分别预测值,取出10个中最大的,也就是神经网络预测的最有可能是什么数字
            _, prediction = torch.max(outputs.data, dim = 1)
            total += labels.size(0)
            correct += (prediction == labels).sum().item()
    print("ACC: %d %%" % (100*correct/total))
    
#7    
if __name__ == '__main__':
    # 设置循环次数
    for epoch in range(5):
        train(epoch)
        test()
```



运行结果

![image-20200906203455952](C:\Users\15117\AppData\Roaming\Typora\typora-user-images\image-20200906203455952.png)





# 用全连接的Linear模型实现

```python
#1
import torch 
import torch.nn as nn
import torch.nn.functional as F 
import torch.utils.data as Data 
import torch.optim as optim
import torchvision

#2
batch_size = 64 
transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.1307, ), (0.3081, ))
])

train_data = torchvision.datasets.MNIST('./dataset/mnist/', train= True, transform= transform)
train_loader = Data.DataLoader(train_data, shuffle= True, batch_size= batch_size)
test_date = torchvision.datasets.MNIST('./dataset/mnist/', train= False, transform= transform)
test_loader = Data.DataLoader(test_date, shuffle= False, batch_size= batch_size)

#3(使用全连接的网络)
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.l1 = nn.Linear(784, 512)
        self.l2 = nn.Linear(512, 256)
        self.l3 = nn.Linear(256, 128)
        self.l4 = nn.Linear(128, 64)
        self.l5 = nn.Linear(64, 10)

    def forward(self, x):
        x = x.view(-1, 784)
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        x = F.relu(self.l3(x))
        x = F.relu(self.l4(x))
        # 注意最后一层不做激活 因为输出要接到后面的softmax里面
        return self.l5(x)
model = Net()

#4
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum= 0.5)

#5
def train(epoch):
    running_loss = 0
    for batch_idx, data in enumerate(train_loader):
        inputs, target = data
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if batch_idx % 300 ==299:
            print("[%d,%5d] loss: %.3f" % (epoch+1, batch_idx+1, running_loss/300))
            running_loss = 0

#6
def test():
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader:
            inputs, labels = data
            outputs = model(inputs)
            _, prediction = torch.max(outputs.data, dim = 1)
            total += labels.size(0)
            correct += (prediction == labels).sum().item()
    print("ACC: %d %%" % (100*correct/total))
    
#7    
if __name__ == '__main__':
    for epoch in range(5):
        train(epoch)
        test()
```

